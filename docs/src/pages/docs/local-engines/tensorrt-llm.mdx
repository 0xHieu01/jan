---
title: TensorRT-LLM
description: A step-by-step guide on customizing the TensorRT-LLM engine.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    TensorRT-LLM Engine,
    TensorRT,
    tensorRT,
    engine,
  ]
---

import { Tabs } from 'nextra/components'
import { Callout, Steps } from 'nextra/components'
import { Settings, EllipsisVertical, Plus, FolderOpen, Pencil } from 'lucide-react'

# TensorRT-LLM
## Overview
Jan uses **TensorRT-LLM** as an optional engine for faster inference on NVIDIA GPUs. This engine uses [Cortex-TensorRT-LLM](https://github.com/janhq/cortex.tensorrt-llm), which includes an efficient C++ server that executes the [TRT-LLM C++ runtime](https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html) natively. It also includes features and performance improvements like OpenAI compatibility, tokenizer improvements, and queues.

<Callout type="info">
Currently only available for **Windows** users, **Linux** support is coming soon!
</Callout>

You can find its settings in **Settings** (<Settings width={16} height={16} style={{display:"inline"}}/>) > **Local Engine** > **TensorRT-LLM**:



## Requirements
- NVIDIA GPU with Compute Capability 7.0 or higher (RTX 20xx series and above)
- Minimum 8GB VRAM (16GB+ recommended for larger models)
- Updated NVIDIA drivers
- CUDA Toolkit 11.8 or newer

<Callout type="info">
For detailed setup guide, please visit [Windows](/docs/desktop/windows#compatibility).
</Callout>

## Engine Version and Updates
- **Engine Version**: View current version of TensorRT-LLM engine
- **Check Updates**: Verify if a newer version is available & install available updates when it's available

## Available Backends

TensorRT-LLM is specifically designed for NVIDIA GPUs. Available backends include:

**Windows**
- `win-cuda`: For NVIDIA GPUs with CUDA support

<Callout type="warning">
TensorRT-LLM requires an NVIDIA GPU with CUDA support. It is not compatible with other GPU types or CPU-only systems.
</Callout>



## Enable TensorRT-LLM 

<Steps>
### Step 1: Install TensorRT-Extension

1. Click the **Gear Icon (‚öôÔ∏è)** on the bottom left of your screen.
2. Select the **TensorRT-LLM** under the **Model Provider** section.
<br/>
![Click Tensor](../_assets/tensor.png)
<br/>
3. Click **Install** to install the required dependencies to use TensorRT-LLM.
<br/>
![Install Extension](../_assets/install-tensor.png)
<br/>
3. Check that files are correctly downloaded.

```bash
ls ~/jan/data/extensions/@janhq/tensorrt-llm-extension/dist/bin
# Your Extension Folder should now include `nitro.exe`, among other artifacts needed to run TRT-LLM
```

### Step 2: Download a Compatible Model

TensorRT-LLM can only run models in `TensorRT` format. These models, aka "TensorRT Engines", are prebuilt for each target OS+GPU architecture.

We offer a handful of precompiled models for Ampere and Ada cards that you can immediately download and play with:

1. Restart the application and go to the Hub.
2. Look for models with the `TensorRT-LLM` label in the recommended models list > Click **Download**.

<Callout type='info'>
  This step might take some time. üôè
</Callout>

![image](https://hackmd.io/_uploads/rJewrEgRp.png)

3. Click **Download** to download the model.

</Steps>