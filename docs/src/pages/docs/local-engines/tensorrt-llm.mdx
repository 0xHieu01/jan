---
title: TensorRT-LLM
description: A step-by-step guide on customizing the TensorRT-LLM engine.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    TensorRT-LLM Engine,
    TensorRT,
    tensorRT,
    engine,
  ]
---

import { Tabs } from 'nextra/components'
import { Callout, Steps } from 'nextra/components'
import { Settings, EllipsisVertical, Plus, FolderOpen, Pencil } from 'lucide-react'

# TensorRT-LLM
## Overview
Jan uses **TensorRT-LLM** as an optional engine for faster inference on NVIDIA GPUs. This engine uses [Cortex-TensorRT-LLM](https://github.com/janhq/cortex.tensorrt-llm), which includes an efficient C++ server that executes the [TRT-LLM C++ runtime](https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html) natively. It also includes features and performance improvements like OpenAI compatibility, tokenizer improvements, and queues.

<Callout type="info">
TensorRT-LLM engine is only available for **Windows** users, **Linux** support is coming soon!
</Callout>


## Requirements
- NVIDIA GPU with Compute Capability 7.0 or higher (RTX 20xx series and above)
- Minimum 8GB VRAM (16GB+ recommended for larger models)
- Updated NVIDIA drivers
- CUDA Toolkit 11.8 or newer

<Callout type="info">
For detailed setup guide, please visit [Windows](/docs/desktop/windows#compatibility).
</Callout>

## Enable TensorRT-LLM 

<Steps>
### Step 1: Install Additional Dependencies
1. Navigate to **Settings** (<Settings width={16} height={16} style={{display:"inline"}}/>) > **Local Engine** > **TensorRT-LLM**:
2. At **Additional Dependencies**, click **Install**

<br/>
![Click Tensor](../_assets/tensorrt-llm-01.png)
<br/>

3. Verify that files are correctly downloaded:
```bash
ls ~/jan/data/extensions/@janhq/tensorrt-llm-extension/dist/bin

# Your Extension Folder should now include `cortex.exe`, among other artifacts needed to run TRT-LLM
```
4. Restart Jan 

### Step 2: Download Compatible Models

TensorRT-LLM can only run models in `TensorRT` format. These models, also known as "TensorRT Engines", are prebuilt specifically for each operating system and GPU architecture.

We currently offer a selection of precompiled models optimized for NVIDIA Ampere and Ada GPUs that you can use right away:

1. Go to **Hub**
2. Look for models with the `TensorRT-LLM` label & make sure they're within your hardware compatibility
3. Click **Download** 

<Callout type="info">
This download might take some time as TensorRT models are typically large files.
</Callout>

<br/>
![Download TensorRT-LLM Model](../_assets/tensorrt-llm-02.png)
<br/>

### Step 3: Start Threads
Once the model(s) is downloaded, start using it in [Threads](/docs/threads)
</Steps>



